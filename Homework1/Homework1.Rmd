
---
title: "Homework 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Question 1 

```{r, include = FALSE}
# library initialization
library(tidyverse)
data_file <- 'https://raw.githubusercontent.com/RoDivinity/DataMining/master/Data/greenbuildings.csv'
data <- read.csv(url(data_file), stringsAsFactors=FALSE)
#tidying the data, classify the size of property into 4 categories
data$size_class <- ifelse (data$size <= 50891, "small" , ifelse
                           (data$size <= 128838, "medium", ifelse
                             (data$size <= 294212, "large", "gigantic")))
#seperate the data based on contract type
data$payment <- ifelse(data$net == 0 , "gross contract" , "net contract")
#seperate the data based on green building type
data$green_type <- ifelse (data$green_rating == 0, 'Non-green' , 'Green')
#cut building's age to 5 year interval
data$trend <- cut(data$age, breaks = c(0 , 10, 20 , 30, 40, 50, 60 , 70 ))
```

I do not agree with the analysis given. If we look at the distribution of rent according to the size of the properties, we observe that the median rate may not vary that much. However, if we seperate the rent based on the type of contract, either gross contract or net contract, there will be discrepancies across median rental rate based on property size. I will classify the size of property based on four quartiles based on the collecting data.

```{r  , echo = FALSE , include = TRUE ,  warning = FALSE , message = TRUE, warning = TRUE }
summary(data$size)
#Median rental rate based on size and payment type
size_rent = ggplot (data, aes(size_class, Rent)) + 
    geom_boxplot( aes(colour = green_type) ) +
    scale_color_manual(values = c( "green" ,  "orange") ) +
    facet_wrap(~ payment) +
  labs ( title = "Rent based on size and contract type" , x = "class size" , y = "Rent")
size_rent

```

As you can observe from the box plot, the medians rental rate between green versus non-green property for large properties ($128838 <x< 294212 \ {feet^{2}}$) do not differ much in net contracts but vary greatly in gross contracts, with green building enjoys around $3.20 in addition. Hence, the rough estimate of difference in rental rate between green and non-green building without factoring in contract type is questionable. 

The next questionable assumption is the rent will be constant throughout the life of the building. 
I will look into the median rent for an interval of every 10 years

```{r echo = FALSE , include = TRUE ,  warning = FALSE , message = TRUE, warning = TRUE}
#Median rent distribution with time of building
age_rent = ggplot(data , aes(trend , Rent)) + geom_boxplot(aes( color = factor(trend) )) + 
  labs(title = "Rental rate trend",
       y = "Rent",
       x = "10 year interval") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6))
age_rent
```

Hence, forecasting the rent to stay constant throughout building's lifetime is irrelevant. Therefore, the revenue stream will not be constant, making the profit prediction after cost repcuperation is dubious. 

## Question 2

```{r , include=FALSE}
#loading library
library(tidyverse)
library(maps)


#Loading data set and airport cross-reference table
data_file <- 'https://raw.githubusercontent.com/RoDivinity/DataMining/master/Data/ABIA.csv'
airport_file <- 'https://raw.githubusercontent.com/RoDivinity/DataMining/master/Data/airports.csv'

#Data for flights
ABIA <- read.csv(url(data_file), stringsAsFactors=FALSE)
#IATA code for airports
airports <- read.csv(url(airport_file), stringsAsFactors=FALSE)

###
#Treatment of data to:
#  + Arrival / Departure
#  + Cancellation / No cancellation flights
#  + Time interval of flight
####
#Create a dummy table for data treatment
hw_ABIA = ABIA
#  + Arrival / Departure
hw_ABIA$ArrDep[hw_ABIA$Dest == "AUS"] = "Arrival"
hw_ABIA$ArrDep[hw_ABIA$Origin == "AUS"] = "Departure"

# + Time interval of flight
hw_ABIA$time = floor(hw_ABIA$DepTime/100)
hw_ABIA$CRStime_Dep = floor(hw_ABIA$CRSDepTime/100)
hw_ABIA$CRStime_Arr = floor(hw_ABIA$CRSArrTime/100)

#  + Cancellation / No cancellation flights
hw_ABIA_fly <- subset(hw_ABIA,(hw_ABIA$Cancelled == "0"))
hw_ABIA_fly <- subset(hw_ABIA_fly,!(hw_ABIA_fly$Diverted == 1))
hw_ABIA_cancel <- subset(hw_ABIA,(hw_ABIA$Cancelled == "1"))


#Looking at US airports IATA code only
US_airport <- subset(airports,(airports$iso_country =="US"))

# data for mapping
###
#     get US airport code for longitude and latitude corresponding to airport name and IATA code
#     there are 2 missing airport in the cross reference table, PHL and PHX

#collect only IATA code, name of airport, and coordinates, only for USA
airports_location = US_airport[ , c("local_code","name","latitude_deg","longitude_deg")]
# add missing value:
airports_location <- rbind(airports_location, c("PHL","Philadelphia International Airport", 39.874400, -75.242400))
airports_location <- rbind(airports_location, c("PHX","Phoenix Sky Harbor International Airport", 33.448400, -112.07400))


#cross reference the IATA code and add in coordination to a new data frame
#add in destination coordinates for airport table
ABIA_Dest = merge(hw_ABIA_fly,airports_location,by.x = "Dest", by.y = "local_code", all.x = TRUE)
colnames(ABIA_Dest)[34:36] <- c("Airport_Destination", "D_lat", "D_long")

#add in arrival coordinates for final airport table
ABIA_All = merge(ABIA_Dest,airports_location,by.x = "Origin", by.y = "local_code", all.x = TRUE)
colnames(ABIA_All)[37:39] <-  c("Airport_Origin", "O_lat", "O_long")

#To avoid error in mapping due to wrong type of data reading as string instead of number
ABIA_All$D_lat <- as.numeric(ABIA_All$D_lat)
ABIA_All$O_lat <- as.numeric(ABIA_All$O_lat)
ABIA_All$D_long <- as.numeric(ABIA_All$D_long)
ABIA_All$O_long <- as.numeric(ABIA_All$O_long)

# grouping flights based on geographical locations of arrival and departure destination
map_airport <- ABIA_All %>%
  group_by(Month, ArrDep, Origin, Dest) %>%
  summarise(O_long = mean(O_long),D_long = mean(D_long),O_lat = mean(O_lat),D_lat = mean(D_lat),airline = length(ArrDep), Delay = mean(DepDelay))

# Preserve the destination and arrival of the flight (which airport coming from and going to)
map_airport$airport[map_airport$ArrDep == "Arrival"] <- map_airport$Origin[map_airport$ArrDep == "Arrival"]
map_airport$airport[map_airport$ArrDep == "Departure"] <- map_airport$Dest[map_airport$ArrDep == "Departure"]

#attach states for origin and destination state to the data frame with coordinates with corresponding original/dest longitude and latitude
map_airport$O_State <- map.where(database = "state", x = map_airport$O_long, y = map_airport$O_lat)
map_airport$D_State <- map.where(database = "state", x = map_airport$D_long, y = map_airport$D_lat)

#Treat the irregularities in data
map_airport$O_State[map_airport$Origin == "BOS"] <- "massachusetts"
map_airport$D_State[map_airport$Dest == "BOS"] <- "massachusetts"
map_airport$O_State[map_airport$Origin == "OAK"] <- "california"
map_airport$D_State[map_airport$Dest == "OAK"] <- "california"

#Matching state with arrival and departure
map_airport$State[map_airport$ArrDep == "Arrival"] <- map_airport$O_State[map_airport$ArrDep == "Arrival"]
map_airport$State[map_airport$ArrDep == "Departure"] <- map_airport$D_State[map_airport$ArrDep == "Departure"]

#Done basic matching data for graphing maps
```

We can look at the volume of flights in and out of Austin to see how the patterns of flights differed state by state

```{r , echo = FALSE, warning=FALSE, message=FALSE}
## start mapping 

# Volume of flights coming in and out of Austin
#make new sum data table
air_map2 <- ABIA_All %>%
  group_by(ArrDep, Origin, Dest) %>%
  summarise(O_long = mean(O_long),D_long = mean(D_long),O_lat = mean(O_lat),D_lat = mean(D_lat),airline = length(ArrDep), DepDelay = mean(DepDelay),ArrDelay = mean(ArrDelay))
air_map2$airport[air_map2$ArrDep == "Arrival"] <- air_map2$Origin[air_map2$ArrDep == "Arrival"]
air_map2$airport[air_map2$ArrDep == "Departure"] <- air_map2$Dest[air_map2$ArrDep == "Departure"]
air_map2$Delay[air_map2$ArrDep == "Departure"] <- air_map2$DepDelay[air_map2$ArrDep == "Arrival"]
air_map2$Delay[air_map2$ArrDep == "Arrival"] <- air_map2$ArrDelay[air_map2$ArrDep == "Departure"]

#plot the graph
p_volume = ggplot(data = air_map2)+
  geom_point(aes(O_long,O_lat , col = airline))+
  geom_point(aes(D_long,D_lat , col = airline))+
  facet_wrap(~ ArrDep)+
  labs(title = "Flights coming in and out of Austin", x = "", y = "")+
  scale_colour_gradient(low="green", high="red")+
  borders("state")+
  coord_quickmap()
p_volume
```

As we observe most of the flights to Austin are coming from Dallas/Fort Worth International, Geogre Bush Intercontinental, Phoenix Sky Harbor and Denver International Airport. Similarly, the departure flights from Austin are coming to the above 5 airports.

To see how the flight pattern changes over the course of the year, let's look at each month in the flight maps of total number of flights arrived to Austin.

```{r , echo=FALSE , message=FALSE, warning=FALSE}
#use basic map table data we have
air_mapArrival = subset(map_airport, map_airport$ArrDep=="Arrival")
air_mapDeparture = subset(map_airport, map_airport$ArrDep=="Departure")

#Arrival flights volume
p_arrival_month = ggplot(data = air_mapArrival)+
  geom_point(aes(O_long,O_lat, col = airline))+
  geom_point(aes(D_long,D_lat , col = airline))+
  scale_colour_gradient(low="green", high="red")+
  borders("state")+
  labs(title = 'Arrival flights by month', x = "", y = "") +
  facet_wrap (~Month ,nrow = 3)
p_arrival_month

#Departure flights volume
p_departure_month = ggplot(data = air_mapDeparture)+
  geom_point(aes(O_long,O_lat, col = airline))+
  geom_point(aes(D_long,D_lat , col = airline))+
  scale_colour_gradient(low="green", high="red")+
  borders("state")+
  labs(title = 'Departure flights by month', x = "", y = "") +
  facet_wrap (~Month ,nrow = 3)
p_departure_month 
```

As we observe, the pattern stays constant throughout the year. The top 5 busiest airports that have arrival and departure flights Austin remain the 5 aforementioned airports.

Besides the traffic of flights, we can investigate how badly the delays of airlines occured based on arrival and departure destinations.

```{r , echo = FALSE,  message=FALSE, warning=FALSE}
#use the sum data table to plot

map_test = ggplot(data = air_map2)+
  geom_point(aes(O_long,O_lat , color = Delay))+
  geom_point(aes(D_long,D_lat , color = Delay))+
  facet_wrap(~ ArrDep)+
  scale_colour_gradient(low="lightgray", high="purple")+
  borders("state")+
  labs(title = "Average delay in minutes", x = " ", y = " ")+
  coord_quickmap()
map_test
```

From the data, we obtain that the worst airport to fly to is DFW, as the average delay is highest amongst all other airports and the volume of flights is constantly high throughout the year, both for arrival and departure flights.

##Question 3

Following the three steps required for each trim:
1. Split the data into a training and a testing set.
2. Run K-nearest-neighbors, for many different values of K, starting at K=2 and going as high as you need to. For each value of K, fit the model to the training set and make predictions on your test set.
3. Calculate the out-of-sample root mean-squared error (RMSE) for each value of K.

```{r , echo=FALSE, include = FALSE}
#loading library
library (mosaic)
library(FNN)
library(tidyverse)

#load data file
data_file <- "https://raw.githubusercontent.com/RoDivinity/DataMining/master/Data/sclass.csv"
sclass <- read.csv(url(data_file), stringsAsFactors=FALSE)

#The variables involved
summary(sclass)

# Focus on 2 trim levels: 350 and 65 AMG
sclass350 = subset(sclass, trim == '350')


sclass65AMG = subset(sclass, trim == '65 AMG')

```


```{r , echo=FALSE}
##                      ###
# First part for trim 350 #
####                     ##

#Training model for 350 trim
#First split the data into training and testing set
N = nrow(sclass350)
N_train = floor(N*.8)
N_test = N - N_train

#####
# Train/test split
#####

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = sclass350[train_ind,]
D_test = sclass350[-train_ind,]

# optional book-keeping step:
# reorder the rows of the testing set by the mileage variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, mileage)


# Now separate the training and testing sets into features (X) = mileage and outcome (y) = price
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

#create a vector to store RMSE for each k 
k_rmse <- rep (0, N_train - 2)

# define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt((sum((y - ypred)^2 ))/N_test)
}

#Unable to run k = 2 
#knn_temp = knn.reg(train = X_train, test = X_test, y = y_train, k=2)

#run a for loop with different value for k
for ( i in 1: (N_train - 2 ) ){
  number =  2 + i
  #print(paste(number))
  knn_temp = knn.reg(train = X_train, test = X_test, y = y_train, k = number)
  #names(knn_temp)
  ypred_knn_temp = knn_temp$pred
  k_rmse[i] <- rmse(y_test, ypred_knn_temp)
}

#Find optimal k 
optimal_k = which.min(k_rmse)
```

The graph of root mean squared error for prediction with various level of k is:

```{r , echo=FALSE}
#Plot the RMSE for different k
plot (k_rmse)
```

Hence, the optimal K from running repeated knn regression from k = 2 to maximum possible k is

```{r , echo=FALSE}
optimal_k + 2
```

The plot for this optimal K = ``r optimal_k +2`` for trim 350 class is as following:

```{r , echo = FALSE}
##PLot knn for optimal k

knn_optimal = knn.reg(train = X_train, test = X_test, y = y_train , k = optimal_k + 2)

ypred_knn_optimal = knn_optimal$pred
D_test$ypred_knn_optimal = ypred_knn_optimal

#plot to compare models
p_test = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') 

#p_test + geom_point(aes(x = mileage, y = ypred_knn_optimal), color='red')
p_test + geom_path(aes(x = mileage, y = ypred_knn_optimal), color='red')
```


Similarly for 65AMG trim we followed the 3 guided steps

```{r , echo = FALSE}

##                         ###
# Second part for trim 65AMG #
####                        ##

#First split the data into training and testing set
N = nrow(sclass65AMG)
N_train = floor(N*.8)
N_test = N - N_train

#####
# Train/test split
#####

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = sclass65AMG[train_ind,]
D_test = sclass65AMG[-train_ind,]

# optional book-keeping step:
# reorder the rows of the testing set by the mileage variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, mileage)

# Now separate the training and testing sets into features (X) = mileage and outcome (y) = price
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

#create a vector to store RMSE for each k 
k_rmse <- rep (0, N_train - 2)

# define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt((sum((y - ypred)^2 ))/N_test)
}

#Unable to run k = 2 
#knn_temp = knn.reg(train = X_train, test = X_test, y = y_train, k=2)


number = 0
#run a for loop with different value for k
for ( i in 1: (N_train - 2 ) ){
  number =  2 + i
  #print(paste(number))
  knn_temp = knn.reg(train = X_train, test = X_test, y = y_train, k = number)
  #names(knn_temp)
  ypred_knn_temp = knn_temp$pred
  k_rmse[i] <- rmse(y_test, ypred_knn_temp)
}
```

We obtain the graph of K versus root mean square error as follow:

```{r , echo = FALSE}
plot (k_rmse)
```

The optimal K for 65AMG trim is
```{r , echo=FALSE}
optimal_k_65AMG = which.min(k_rmse)
optimal_k_65AMG = optimal_k_65AMG + 2
```

For this optimal K = ``r optimal_k_65AMG +2``, we have the plot of fitted model:

```{r, echo=FALSE}
##PLot knn for optimal k

knn_optimal = knn.reg(train = X_train, test = X_test, y = y_train , k = optimal_k + 2)

ypred_knn_optimal = knn_optimal$pred
D_test$ypred_knn_optimal = ypred_knn_optimal

#plot to compare models
p_test = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') 


#p_test + geom_point(aes(x = mileage, y = ypred_knn_optimal), color='red')
p_test + geom_path(aes(x = mileage, y = ypred_knn_optimal), color='red')
```

Since the process of splitting data into training and testing set is random, the optimal K for each trim changes from time to time. For each specific run, it is hard to tell which optimal K going to be larger. However, on average, optimal K for trim 350 class should be smaller than trim 65 AMG since there are more data for 350 class than 65 AMG.
