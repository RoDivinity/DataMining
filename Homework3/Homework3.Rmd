---
title: "Untitled"
author: "Linh Nguyen"
date: "4/7/2019"
output: pdf_document
---
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(gamlr)
library(dplyr)
```

# Exercise 3
## Exercise 3.1
### Build the best predictive model possible for price.
```{r setup_3.1, echo=FALSE, warning=FALSE}
urlfile<-'https://raw.githubusercontent.com/RoDivinity/DataMining/master/Data/greenbuildings.csv'
gb<-read.csv(url(urlfile))
#clean data
gb =na.omit(gb)

#allow lasso regression to converge
gb$size = gb$size/1000

#clean data by deleting the data with occupacy rate equal to 0%
gb <- subset(gb,(gb$leasing_rate != 0))
```

I preprocess the data as follow:

  1. Clear out all the observations with missing values in the data. 
  
  2. Scale down the size by $\frac{1}{1000}$ to abide the lasso computation's requirement so it can converge. 
  
  3. Omit building with leasing rate = 0 because it makes no sense to charge a rent for unoccupied buildings (they are either under construction or abandoned or other reasons)
  

To find best predictive model, I will deploy stepwise regression and lasso regression.
  a. Stepwise regression using LEED and EnergyStar explicitly and using green rating (implying LEED and Energy Star is green certified). In both models, I started with the null model by regressing rent on one, followed by adding new variables in the forward selection method. This is the baseline model to run stepwise regression

Model using LEED and EnergyStar explicitly:

```{r stepwise with LEED & Energy, echo=FALSE, warning=FALSE}
## stepwise model with LEED & Energy (model 1)
full = lm(Rent ~ .-CS_PropertyID-green_rating, data=gb)
null = glm(Rent~1, data=gb)
fwd = step(null, scope=formula(full), dir="forward", trace = FALSE)
big  = lm(Rent ~ (.-CS_PropertyID-green_rating)^2, data=gb)
stepwise = step(null, scope=formula(big), dir="both", trace = FALSE)
#45 used, null = Forward, then stepwise 
model1 = formula(stepwise)
model1
```

Model using green certified category:
```{r model2, echo=FALSE, warning=FALSE}
## stepwise with green rating (model 2)
full = lm(Rent ~ .-CS_PropertyID-LEED-Energystar, data=gb)
null = lm(Rent~1, data=gb)
fwd = step(null, scope=formula(full), dir="forward", trace = FALSE)
big  = lm(Rent ~ (.-CS_PropertyID-LEED-Energystar)^2, data=gb)
stepwise = step(null, scope=formula(big), dir="both", trace = FALSE)
#44 used, null = Forward, then stepwise 
model2 = formula(stepwise)
model2
```

  b. Lasso regression using LEED and EnergyStar separately, then second lasso regression combining them into a single "green certified" category. I created sparse matrix including all interactions terms then run lasso regression on it


```{r model3_lasso, echo=FALSE, warning=FALSE}
## Gamma Lasso with LEED & Energy (model 3)
gbx = sparse.model.matrix(Rent ~ (.-CS_PropertyID-green_rating)^2, data=gb)[,-1] 
gby = gb$Rent
gblasso = gamlr(gbx, gby, lambda.min.ratio=0.00001)
plot(gblasso) # the path plot!
min1 <- which.min(AICc(gblasso))
```
Minimum AIC occurs at segment `r min1`.

Thus, coefficients chose in the model are:
```{r model3, echo=FALSE, warning=FALSE}
gbbeta = coef(gblasso)
# 184 used in Lasso
# sum(gbbeta!=0)
p1 <- dimnames(gbbeta)[[1]]
p2 <- c()
for (i in c(1:length(gbbeta))){
  p2 <- c(p2, as.list(gbbeta)[[i]])
}
model3 = c("Rent ~ ")
for (i in c(2:length(gbbeta))){
  if (p2[i] != 0){
    if (model3 == "Rent ~ "){
      model3 = paste(model3, p1[i])
    }
    else{
      model3 = paste(model3,"+", p1[i])
    }
  }
}
model3 <- as.formula(model3)
model3
#size <- length(p2)
#size
```



```{r model4_lasso, echo=FALSE, warning=FALSE}
## Gamma Lasso model 2.2 Green-rating
gbx = sparse.model.matrix(Rent ~ (.-CS_PropertyID-LEED-Energystar)^2, data=gb)[,-1] 
gby = gb$Rent
gblasso = gamlr(gbx, gby, lambda.min.ratio=0.00001)
plot(gblasso) # the path plot!
min2 <- which.min(AICc(gblasso))
```
Minimum AIC occurs at segment `r min2`.

Coefficients chose in the model are:

```{r model4, echo=FALSE, warning=FALSE}
gbbeta2 = coef(gblasso)
p1 <- dimnames(gbbeta2)[[1]]
p2 <- c()
for (i in c(1:length(gbbeta2))){
  p2 <- c(p2, as.list(gbbeta2)[[i]])
}
model4 = c("Rent ~ ")
for (i in c(2:length(gbbeta2))){
  if (p2[i] != 0){
    if (model4 == "Rent ~ "){
      model4 = paste(model4, p1[i])
    }
    else{
      model4 = paste(model4,"+", p1[i])
    }
  }
}
model4 <- as.formula(model4)
model4
#size2 <- length(p2)
#size2
```

  c. To measure predictive power, I employed k-fold cross validation. 
I set k = 15 and calculated 4 models' CVs. Stepwise models has smaller CVs compared to Lasso models. Between 2 stepwise model, the one with a single green certified category has lower CV.

In conclusion, the stepwise model with "green certified" category had the minimum CV, and therefore it is the best predictive model possible for rent price.

```{r RMSE_comparison, echo=FALSE, warning=FALSE}
N = nrow(gb)
# Create a vector of fold indicators
K = 15
fold_id = rep_len(1:K, N)  # repeats 1:K over and over again
fold_id = sample(fold_id, replace=FALSE) # permute the order randomly
step_err_save = rep(0, K)
step_err_save2 = rep(0, K)
lasso_err_save = rep(0, K)
lasso_err_save2 = rep(0, K)
for(i in 1:K) {
  train_set = which(fold_id != i)
  y_test = gb$Rent[-train_set]
  step_model = lm(model1, data=gb[train_set,])
  step_model2 = lm(model2, data=gb[train_set,])
  lasso_model = lm(model3, data=gb[train_set,])
  lasso_model2 = lm(model4, data=gb[train_set,])
  
  yhat_test1 = predict(step_model, newdata=gb[-train_set,])
  step_err_save[i] = mean((y_test - yhat_test1)^2)
  
  yhat_test2 = predict(step_model2, newdata=gb[-train_set,])
  step_err_save2[i] = mean((y_test - yhat_test2)^2)
  
  yhat_test3 = predict(lasso_model, newdata=gb[-train_set,])
  lasso_err_save[i] = mean((y_test - yhat_test3)^2)
  
  yhat_test4 = predict(lasso_model2, newdata=gb[-train_set,])
  lasso_err_save2[i] = mean((y_test - yhat_test4)^2)
}
# RMSE
c(sqrt(mean(step_err_save)),sqrt(mean(step_err_save2)),sqrt(mean(lasso_err_save)),sqrt(mean(lasso_err_save2)))
```

### Use this model to quantify the average change in rental income per square foot (whether in absolute or percentage terms) associated with green certification, holding other features of the building constant.

```{r result, echo=FALSE, warning=FALSE}
step_model2 = lm(model2, data=gb)
temp = coef(step_model2) %>% round(3)
temp 
```

Holding all other significant features of the building fixed, green certified (LEED and EnergyStar combined) buildings are expected to be `r temp["green_rating"]` $\$/ft^{2}$ per year more expensive than non-green buildings.

### Assess whether the "green certification" effect is different for different buildings, or instead whether it seems to be roughly similar across all or most buildings.

  As mentioned, the stepwise model with collapsed category of green certified has the best predictive power. Green certification buildings with amenities is `r temp["green_rating:amenities"]` $\$/ft^{2}$ per year less than green certification buildings without amenities. Hence, "green certification" effect is different for buildings of with and without amenities. 

  One possible explanation is that the green buildings with amenities are normally considered as commercial buildings, so the buildings need to pay the energy fee according to different scheme compared to residential buildings. It can be that commercial rate is higher than residential rate. Thus, residents in the green buildings with amenities must pay more than those in the green buildings without amenities. Therefore green buildings with amenities have incentive to lower the rent fee to captivate more potential renters.

## Question 2

### Why can’t I just get data from a few different cities and run the regression of “Crime”on “Police” to understand how more cops in the streets affect crime? (“Crime” refersto some measure of crime rate and “Police” measures the number of cops in a city.)  
Because there is an innate correlation between "Crime" and "Police". As a city with more crime rate, there is an incentive to accommodate a larger police force. This is the problem of endogeneity. To be more specific, the endogeneity problem arises because of simultaneity.

### How were the researchers from UPenn able to isolate this effect? Briefly describetheir approach and discuss their result in the “Table 2” below, from the researcher's paper.  
They use instrumental variables, in this case the terrorism alert system. Since terrorism alert system is a random event, so it must be the case that the high-alert days is uncorrelated with the residual from regression of "Crime" on "Police". However, high-alert days require extra police force, so it is correlated with the number of police (i.e. on high-alert days, there is more police). Hence, using this instrumental variable will solve the problem of endogeneity.
In addition, they control for Metro ridership to make the comparison more compatible.


### Why did they have to control for Metro ridership? What was that trying to capture?  
They must control for ridership because they do not want to overestimate the effect of police on crime reduction. It is possible on high-alert days, citizens are afraid to go out so less likely for a crime to happen, so the regression may over estimate the effectiveness of police on crime reduction. It was trying to capture the amount of potential victims/offenders of street crime are the same for high-alert days and non high-alert days.

### Below I am showing you "Table 4" from the researchers' paper.  Just focuson the first column of the table. Can you describe the model being estimated here? What is the conclusion?
The model is estimating the effect on crime reduction by increasing police force between districts. The model in estimation is crime rate on high-alert days with interactions with district with police patrol, controlling for Metro ridership. As we observe, crime rate is reduced, both on high-alert days (more police force is deployed) and within the first police district area (more immediate coverage of police force). For other district (implying less police force available to monitor), the effectiveness of crime rate reduction is not significant. Hence, we can conclude more police force will reduce crime rate.



